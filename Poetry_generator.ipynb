{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "a generative poetry system that learns from your uploaded PDF."
      ],
      "metadata": {
        "id": "wxmryRk6gK6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl\n",
        "!pip install transformers datasets torch PyMuPDF accelerate\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import fitz\n",
        "import tempfile\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Basic text cleaning (adjust for your needs)\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return text\n",
        "\n",
        "def load_pdf_data(uploaded):\n",
        "    text = \"\"\n",
        "    for fn in uploaded.keys():\n",
        "        with fitz.open(fn) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def build_text_dataset(text, tokenizer, block_size=128):\n",
        "    # Create a temporary file to store the text\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as tmp_file:\n",
        "        tmp_file.write(text)\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=tmp_file_path,  # Use the temporary file path\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    return dataset, data_collator\n",
        "\n",
        "def train_model(dataset, data_collator, model):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=100,  # Adjust as needed\n",
        "        per_device_train_batch_size=4,  # Adjust as needed\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "import torch\n",
        "\n",
        "def generate_poem(model, tokenizer, prompt=\"\", max_length=100):\n",
        "    # Ensure model and input are on the same device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)  # Move model to the device\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device) # Move input to device\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    poem = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return poem\n",
        "\n",
        "# Upload PDF\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load and preprocess data\n",
        "text = load_pdf_data(uploaded)\n",
        "dataset, data_collator = build_text_dataset(text, tokenizer)\n",
        "\n",
        "# Train the model\n",
        "train_model(dataset, data_collator, model)\n",
        "\n",
        "# Generate a poem based on the uploaded PDF\n",
        "generated_poem = generate_poem(model, tokenizer, prompt=\"The moon shines dimly, weeping in the sky for the fallen, shards of light piercing dark dimensions\")\n",
        "print(generated_poem)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UoCDqyglgK6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1. **Libraries:**\n",
        "    - `transformers`: Provides pre-trained language models (like GPT-2) and utilities for fine-tuning.\n",
        "    - `datasets`: Facilitates loading and processing text data for training.\n",
        "    - `torch`: The underlying deep learning framework for Transformers.\n",
        "\n",
        "2. **Model Setup:**\n",
        "    - Loads GPT-2 tokenizer and model (pre-trained on a large corpus of text).\n",
        "\n",
        "3. **Data Preparation:**\n",
        "    - `load_pdf_data`: Extracts text from the uploaded PDF.\n",
        "    - `build_text_dataset`: Creates a dataset from the text for training.\n",
        "\n",
        "4. **Model Training:**\n",
        "    - Fine-tunes the GPT-2 model on the dataset derived from your PDF. This teaches the model the specific style and patterns of your poetry.\n",
        "\n",
        "5. **Poem Generation:**\n",
        "    - `generate_poem`: Uses the fine-tuned model to generate a new poem.\n",
        "        - You can provide an optional `prompt` to guide the poem's theme or starting point.\n",
        "        - `max_length` controls the maximum length of the generated poem.\n",
        "\n",
        "**How to Use:**\n",
        "\n",
        "1. **Upload your PDF** of poems in Colab.\n",
        "2. **Run the code.** It will fine-tune GPT-2 on your poems.\n",
        "3. **A poem in the style of your PDF will be printed.**  Feel free to experiment with different prompts!\n",
        "\n",
        "**Key Improvements:**\n",
        "\n",
        "*   **Uses GPT-2:** A powerful language model known for generating creative text.\n",
        "*   **Fine-tuning:** Adapts the model to your specific writing style.\n",
        "*   **Flexibility:** Allows you to provide a prompt to guide the generated poem.\n",
        "\n",
        "\n",
        "Let me know if you have any other questions."
      ],
      "metadata": {
        "id": "-AzaqZ8UgK6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/mhadihossaini/Custom_GPT2_Text_Generation\">https://github.com/mhadihossaini/Custom_GPT2_Text_Generation</a></li>\n",
        "  <li><a href=\"https://discuss.huggingface.co/t/gpt2-training-from-scratch-in-german/1157\">https://discuss.huggingface.co/t/gpt2-training-from-scratch-in-german/1157</a></li>\n",
        "  <li><a href=\"https://blog.devgenius.io/build-your-own-llm-model-using-openai-dd2be7fe9bb2\">https://blog.devgenius.io/build-your-own-llm-model-using-openai-dd2be7fe9bb2</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "iWvyRUKSgK6c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}