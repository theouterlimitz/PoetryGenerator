{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theouterlimitz/PoetryGenerator/blob/main/Rap_Shakespeare_Style_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Here's the updated code that trains on both Shakespearean sonnets and rap lyrics, removing the beam search and temperature scaling modifications:"
      ],
      "metadata": {
        "id": "4VQfQpCp0Z0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl\n",
        "!pip install transformers datasets torch PyMuPDF accelerate\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import fitz\n",
        "import tempfile\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return text\n",
        "\n",
        "def load_pdf_data(uploaded):\n",
        "    text = \"\"\n",
        "    for fn in uploaded.keys():\n",
        "        with fitz.open(fn) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def build_text_dataset(text, tokenizer, block_size=128):\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as tmp_file:\n",
        "        tmp_file.write(text)\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=tmp_file_path,\n",
        "        block_size=block_size,\n",
        "    )\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    return dataset, data_collator\n",
        "\n",
        "def train_model(dataset, data_collator, model):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=200,\n",
        "        per_device_train_batch_size=4,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "def generate_poem(model, tokenizer, prompt=\"\", max_length=1000, num_return_sequences=1): # Added num_return_sequences with default value 1\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    # If you want to generate multiple sequences, uncomment the line below and set num_beams > 1\n",
        "    # output = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences, num_beams=5)\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences)\n",
        "    poem = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return poem\n",
        "\n",
        "# Upload Shakespeare Sonnets PDF\n",
        "from google.colab import files\n",
        "uploaded_sonnets = files.upload()\n",
        "\n",
        "# Load and preprocess Shakespeare Sonnets data\n",
        "sonnets_text = load_pdf_data(uploaded_sonnets)\n",
        "sonnets_text = preprocess_text(sonnets_text)\n",
        "\n",
        "# Upload Rap Lyrics PDF\n",
        "uploaded_lyrics = files.upload()\n",
        "\n",
        "# Load and preprocess Rap Lyrics data\n",
        "rap_lyrics_text = load_pdf_data(uploaded_lyrics)\n",
        "rap_lyrics_text = preprocess_text(rap_lyrics_text)\n",
        "\n",
        "# Combine Datasets\n",
        "combined_text = sonnets_text + rap_lyrics_text\n",
        "\n",
        "# Build Dataset and Data Collator\n",
        "dataset, data_collator = build_text_dataset(combined_text, tokenizer)\n",
        "\n",
        "# Train the Model\n",
        "train_model(dataset, data_collator, model)\n",
        "\n",
        "# Generate a poem based on the combined dataset\n",
        "generated_poem = generate_poem(model, tokenizer, prompt=\"The moon shines dimly, weeping in the sky for the fallen, shards of light piercing dark dimensions\")\n",
        "print(generated_poem)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "746qHbXG0Z0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Changes:**\n",
        "\n",
        "- **Removed Beam Search and Temperature Scaling:** The `generate_poem` function now uses the default greedy decoding strategy, as the `num_beams` and `temperature` parameters have been removed."
      ],
      "metadata": {
        "id": "LSzEMw8s0Z0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://github.com/mhadihossaini/Custom_GPT2_Text_Generation\">https://github.com/mhadihossaini/Custom_GPT2_Text_Generation</a></li>\n",
        "  <li><a href=\"https://github.com/MeLLL-UFF/tuning_sentiment\">https://github.com/MeLLL-UFF/tuning_sentiment</a> subject to MIT</li>\n",
        "  <li><a href=\"https://discuss.huggingface.co/t/gpt2-training-from-scratch-in-german/1157\">https://discuss.huggingface.co/t/gpt2-training-from-scratch-in-german/1157</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "2aCSgY7s0Z0x"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}